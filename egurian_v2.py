# -*- coding: utf-8 -*-
"""EGurian

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c9v9C_wCN36kVeQeZGOypqkwsAY1WT0d
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

def PoliRegFit(X,y, grau, pesos):
  # função feita para fazer ajuste polinomial univariada
  # recebe os valores de X - condições
  # reecebe y - resultados experimentais

  # montagem da matriz posto para o grau polinomial selecionado
  mat_X = np.array([X**i for i in range(0,grau+1)]).T

  # se a matriz de pesos existir, o algoritmo leva em consideração a matriz existente, realizando um WLS (Weigted Last Squares)
  if pesos.any() == True:
    # montagem da matriz de pesos, se houverem
    W = np.array([[pesos[i] if i ==j else 0 for i in range(0,len(pesos))] for j in range(0,len(pesos))])
    # cálculo dos coeficientes
    b = np.linalg.inv(mat_X.T.dot(W).dot(mat_X)).dot(mat_X.T).dot(W).dot(y)

  # se a matriz de pesos não existir, o algoritmo realisa um OLS (Ordinary Last Squares)
  else:
    # cálculo dos coeficientes
    b = np.linalg.inv(mat_X.T.dot(mat_X)).dot(mat_X.T).dot(y)




  return(b)

def LOESS(X, y, novo, janela, grau):
  #função feita para realizar a suavização de espectros pelo método da interpolação local pelo método dos mínimos quadrados ponderados ponderada
  # X = eixo X original
  # y = valores de y ao longo do eixo X
  # len(X) == len(y) - IMPORTANTE
  # novo = novo eixo X - MUIDADO: o passo do eixo pode mudar mas a janela deve ser a mesma, ou seja, os valores de inicio e final devem corresponder ao eixo X original
  # janela = a janela de valores usada para realizar a regressão polinomial local, deve ser um valor ímpar >= 15
  # grau = grau do polinômio usado para mensurar o novo valor de y

  # comando de repetição que percorre todos os valores do novo eixo X (alvo da interpolação)
  # normalisado = eixo_X_original - i
  # normalisado_mod = distância entre os pontos do eixo X e o valor i
  # indice = índice do ponto de X mais próximo de i
  # novo_y = novos valores do eixo y
  novo_y = list()
  novo_x = list()

  for i in novo:
    temporario = np.array(range(0,janela))
    centro = int(np.median(temporario))

    normalisado = X - i
    normalisado_mod = np.abs(normalisado)
    indice = np.where(normalisado_mod == normalisado_mod.min())[0][0]

    # o índice da menor distância deve ter valor maior ou igual o ponto médio da janela (se janela ==17, indice >=8)
    # slicey = fatia do eixo y usada na interpolação
    # slice_normalisado = slice do eixo X normalisado em i usado para a interpolação
    # slice_normalisado_mod = slice das distâncias dos pontos do eixo X até i
    # dist_máx = maior distância entre o eixo X e o valor i dentro do slice de interpolação
    # W = pesos usados para o WLS (weigted last squares)
    if indice >= centro and indice < len(X) - centro:
      novo_x.append(i)
      slicey = y[indice -centro :indice + centro +1]
      slice_normalisado = normalisado[indice -centro :indice + centro +1]
      slice_normalisado_mod = normalisado_mod[indice -centro :indice + centro +1]
      dist_max = slice_normalisado_mod.max()


      W = list()


      # cálculo dos pesos através da fórmula 1 - (((|Xi - i|)/d_max))**3)**3
      for j in slice_normalisado_mod:
        if j < dist_max:
          w = (1-((j/dist_max)**3))**3
        else:
          w = 0
        W.append(w)

      # cálculo da WLS
      # b = coeficientes obtidos através da WLS
      # y0 intercepto obtido através da WLS, que é o resultado da interpolação no ponto i
      W = np.array(W)
      b = PoliRegFit(slice_normalisado, slicey, grau, W)
      y0 = b[0]
      novo_y.append(y0)


  return(novo_x, novo_y)

def BasePolyCor(X, y, grau):
  # rotina de cálculos para ajustar o baseline do espectro
  # X = comprimentos de onda do espectro (faixa espectral)
  # y = sinais obtidos em cada comprimento de onda
  # len(X) == len(y) - IMPORTANTE
  # grau = grau do polinômio usado para calcular a baseline

  # matriz aonde os sucessivos resultados da regressão são armazenados (inativo na versão final por conta de memória)
  #historico = list()

  # PRIMEIRO CÁLCULO DA BASELINE
  # peso = vetor com len(y) numeros de elementos, todos com valor 1
  # b = coeficientes de regressão calculados através da função PoliRegFit
  # base = baseline calculado ponto a ponto através dos coeficientes obtidos com PolyRegFit e os valores de X
  peso = np.ones(len(y))
  b = PoliRegFit(X, y, grau, peso)
  base = np.array([np.array([b[i]*j**i for i in range(0,len(b))]).sum() for j in X])

  # inserção do espectro na lista de valores (inativo na versão final por conta de memória)
  #historico.append(base)

  # r = resíduos do modelo (valores obtidos experimentalmente(y) - valores calculados com o polinômio(base))
  # r_mod = módulos dos resíduos
  # mediana = mediana dos módulos dos resíduos (condição adicionada para evitar divisão por 0)
  # r_norm = resíduos normalisados na mediana dos módulos dos resíduos
  r = y - base
  r_mod = np.abs(r)
  mediana = np.median(r_mod)
  if mediana !=0: r_norm = r/mediana
  else: r_norm = r/0.001

  # peso = novo cálculo dos pesos, mas dessa os valores a baixo da curva são pesados com 1 (máximo de peso) e os valores a baixo são pesados em e**-(2*resíduo normalisado)
  #        a idéia é que valores acima da curva tenham baixa significância no cálculo, e valores a baixo tenham maior significância
  # b = novos coeficientes calculados com os pesos - é o reajuste da linha de base
  # base_nova = novo baseline obtido aplicando a nova equação em cada ponto espectral
  peso = np.array([ 1/(1 + np.exp(i)) if i >  0 else 1  for i in r_norm])
  b = PoliRegFit(X, y, grau, peso)
  base_nova = np.array([np.array([b[i]*j**i for i in range(0,len(b))]).sum() for j in X])
  #historico.append(base_nova)

  # looping estabelecido para sucessivos cálculos de pessagem dos valores e sucessivas regressões com os novos pesos
  # o looping continua enquanto norma(base_nova - base)/norma(base_nova) > 0.01, ou seja, o looping só para quando a diferença da norma da base line calculada e a anterior
  # for menor que 1%, indicando que não haverá ganho analitico significante em novos cálculos
  while np.linalg.norm(base_nova-base)/np.linalg.norm(base_nova) > 0.01:

    # novo cálculo de pesos com a mesma metodologia
    r = y - base_nova
    r_mod = np.abs(r)
    mediana = np.median(r_mod)
    if mediana !=0: r_norm = r/mediana
    else: r_norm = r/0.001
    peso = np.array([ 1/(1 + np.exp(i)) if i >  0 else 1  for i in r_norm])

    # novo cálculo do polínômio e baseline com a mesma metodologia, mas antes armazena a baseline antiga no valor "base"
    b = PoliRegFit(X, y, grau, peso)
    base = base_nova
    base_nova = np.array([np.array([b[i]*j**i for i in range(0,len(b))]).sum() for j in X])
    # inserção dos novos valores calculanos no histórico (inativo na versão final por conta de memória)
    #historico.append(base_nova)

  # PASSO CHAVE: cálculo dos espétros com baseline corrigido
  novo_espectro = y - base_nova

  # comando para plotar as sucessivas regressões e o espectro com baseline corrigida (inativo na versão final por conta de memória)
  #figura, axis = plt.subplots(2, 1, figsize = (13,10))
  #axis[0].plot(X, y)
  #axis[1].plot(X, novo_espectro)
  #for i in historico:
  #  axis[0].plot(X, i)

  return(novo_espectro)

def LDAcalc(X, classe, pcs):
  # X deve ser a matriz de scores contendo as componentes principais nas colunas e as observações (amostras) nas linhas
  # classe é a a varíável que classifica as observações, no exemplo em questão a variável é "class", que classifica as observações em: CTR ou H0T
  # classe: nada mais é que uma variável qualitativa
  # pcs são as componentes principais usadas na LDA ("linear discriminant analysis")

  # Uc - matriz com as médias das scores de cada PC para cada grupo da classe escolhida, se a classe tiver 5 grupos a matriz Uc terá 5linhas e
  # uma quantidade de colunas igual ao número de PC's escoilhidas no input inicial
  # Ugeral - ma média de scores para todas as amostras em cada PC, é um vetor com pcs elementos, número de elementos igual a quantidade de pcs escolhidas
  # grupos - lista com o nome dos grupos da classe escolhida
  # Ut - lista aonde sesão armezenadas as amostras escolhidas com as respectivas PC's, separadas por grupos
  #=============================================================================================
  Uc = X.groupby(classe).mean().iloc[:,pcs]
  Ugeral = Uc.mean()
  grupos = Uc.index
  Ut = list()


  # armazenamento dos grupos de amostras com suas respectivas PC's na lista Ut, separadamente
  #=============================================================================================
  for i in grupos:
    Ut.append(X[X[classe] == i].iloc[:,pcs].to_numpy())

  # Dw - cálculo (xi - uc), aonde xi é a amostra contendo n PC's (escolhidas no input pcs) e uc é a média do valor de cada PC naquele grupo
  # Sw - cálculo (xu - u), aonde xu é a média das PC's em cada grupo de amostras e u é a média geral da PC para todas as amostras
  #=============================================================================================
  if len(pcs) == 1: Dw = [np.array([ j - Uc.iloc[i,0] for j in Ut[i]]) for i in range(0,len(Ut))]
  else: Dw = [np.array([ j - Uc.iloc[i,:] for j in Ut[i]]) for i in range(0,len(Ut))]
  Db =np.array([Uc.loc[i] - Ugeral for i in Uc.index])

  # cálculo de produto externo para cada amostra em cada grupo, gera n matrizes com pcs linhas e pcs colunas
  # posteriormente realiza a soma termo a termo de todas as matrizes obtidas, obtendo uma matriz com pcs linhas e pcs colunas
  # operação S(xi - uc)(xi - uc).T
  #=============================================================================================
  Sw = np.array(0)
  for i in Dw:
    for d in i:
      s = np.array([[w * j for j in d] for w in d])
      Sw = Sw + s

  # cálculo do produto externo dos resultados das subtrações das médias das PC's para cada grupo pela média geral, gera uma matriz com pcs linhas e pcs colunas
  # operação S(xu - u)(xu - u).T
  #=============================================================================================
  Sb = np.array(0)
  for i in range(0,len(grupos)):
    n = len(X[X[classe] ==grupos[i]])
    s = n*np.array([[w * j for j in Db[i]] for w in Db[i]])
    Sb = Sb + s

  # A = multiplicação matricial da inversa de Sw por Sb
  # valores = autovalores de A
  # vetores = autovetores de A - chave para o cálculo das discriminantes lineares
  # LDA = dataframe com as discriminantes lineares
  # IMPORTANTE: os autovetores mais importantes são aqueles com maiores autovalores
  # IMPORTANTE: O número de LDA's levados em conta é igual a C-1 = número de grupos na classe -1
  #=============================================================================================
  A = np.linalg.inv(Sw).dot(Sb)
  valores, vetores = np.linalg.eig(A)
  LDA = pd.DataFrame(data = np.real(vetores).T, index = np.real(valores))
  LDA.sort_index(ascending= False, inplace = True)

  # para obter o valor discriminante em cada amostra a partir das PC's deve-se: Matriz_PCs.dot(LDA.iloc[0])
  # ou seja, multiplicar a matriz contendo as amostras e as PC's usadas no calculo pela primeira linha da matriz LDA - a primeira linha contem
  # as discriminantes mais relevantes

  return(Sb, Sw, LDA)

def sep_sort(X, grupos, classe):
  classes = X[classe].unique()
  lista_classes = list([] for i in range(grupos))
  dict_classes = {c: X[X[classe]==c].index.values for c in classes}
  for c in dict_classes:
    np.random.shuffle(dict_classes[c])
    for idx, i in enumerate(dict_classes[c]):
      lista_classes[idx % grupos].append(i)

  return(lista_classes)

def AUC(y_true, scores):

  ps_scores = scores[y_true == 1].to_numpy()
  ng_scores = scores[y_true == 0].to_numpy()

  cont = 0.0
  for ps in ps_scores:
    cont = cont + np.sum(ps>ng_scores)
    cont = cont + (0.5 * np.sum(ps == ng_scores))

  AUC = cont /(len(ps_scores)*len(ng_scores))

  return(AUC)

def RDVC3(X, exte, inte, classe, n_PC):
  classes = X[classe].unique()
  classe_sup = classes[1]

  #segmentação das amostras em 3 grupos  do ciclo externo
  lista_grupos_ext = sep_sort(X, exte, classe)
  lista_scores = list()
  lista_LDA = list()
  lista_confusao = list()
  lista_parametros = list()
  n_pcs = list()
  historico_erros = list()
  historico_AUC = list()
  # início dos ciclos externos
  # em cada ciclo um fold externo é escolhido como teste e os outros 2 são escolhidos como treino
  for ex in range(len(lista_grupos_ext)):
    teste = lista_grupos_ext[ex]
    treino = np.hstack([ lista_grupos_ext[j] for j in range(len(lista_grupos_ext)) if j !=ex ])

    # segmentação do grupo de treino em 7 grupos internos
    lista_grupos_int = sep_sort(X.loc[treino], inte, classe)

    # inicio dos ciclos internos
    erro_por_ciclo = list([] for g in range(0,n_PC))
    AUC_por_ciclo = list([] for g in range(0,n_PC))
    for it in range(len(lista_grupos_int)):
      validacao = lista_grupos_int[it]
      avaliacao = np.hstack([lista_grupos_int[h] for h in range(len(lista_grupos_int)) if h!=it])

      df_avaliacao = X.loc[avaliacao, :]
      df_validacao = X.loc[validacao, :]
      y_avaliacao = df_avaliacao[classe]
      y_validacao = df_validacao[classe]
      df_avaliacao = df_avaliacao.drop(classe, axis = 1)
      df_validacao = df_validacao.drop(classe, axis = 1)
      mu = df_avaliacao.mean()
      df_avaliacao_c = df_avaliacao - mu
      df_validacao_c = df_validacao - mu

      U, s, Vh = np.linalg.svd(df_avaliacao_c, full_matrices= False)
      V = Vh.T
      scores_avaliacao = df_avaliacao_c.dot(V[:,0:n_PC])
      scores_avaliacao_df = pd.DataFrame(scores_avaliacao)
      scores_avaliacao_df[classe] = y_avaliacao.values
      scores_validacao = df_validacao_c.dot(V[:,0:n_PC])

      # inicio dos ciclos de cálculo de AUC variando a quantidade de PC's para a escolha da quantidade ótima de PC's
      for pc in range(n_PC):
        k = pc + 1
        PC = list(range(pc+1))

        Sb, Sw, LDA = LDAcalc(scores_avaliacao_df, classe, PC)
        w = LDA.iloc[0,:]
        LDA_avaliacao = scores_avaliacao_df.iloc[:,0:k].dot(w)
        true_avaliacao = (y_avaliacao == classe_sup).astype(float).to_numpy()
        if np.mean(LDA_avaliacao[true_avaliacao == 1]) < np.mean(LDA_avaliacao[true_avaliacao == 0]): w = -w

        LDA_validacao = scores_validacao.iloc[:,0:k].dot(w)
        y_true = (y_validacao == classe_sup).astype(float).to_numpy()
        auc = AUC(y_true, LDA_validacao)

        erro = 1 - auc
        erro_por_ciclo[pc].append(erro)
        AUC_por_ciclo[pc].append(auc)
        #FINAL DO CICLO INTERNO E OTIMIZAÇÃO DA QUANTIDADE DE pcs USADAS NO CICLO EXTERNO EXT

    historico_erros.append(erro_por_ciclo)
    historico_AUC.append(AUC_por_ciclo)
    AUC_medias = [np.mean(AUC_por_ciclo[i]) for i in range(len(AUC_por_ciclo))]
    AUC_medias = np.array(AUC_medias)
    PC_otima = np.argmax(AUC_medias)

    desv_pad = np.std(AUC_por_ciclo[PC_otima])
    limiar_PC = AUC_medias.max() - (desv_pad/np.sqrt(inte))
    PC_usada = np.where(AUC_medias >= limiar_PC)[0][0]
    n_pcs.append(PC_usada +1)
    #print(f" no ciclo externo {ex + 1 } a quantidade de PC's com maior AUC foi {PC_otima+1}\n"
    #      f"o valor AUC dessas PC's foi {AUC_medias.max()}\n"
    #       f"o limiar obtido foi{limiar_PC}\n"
    #       f"a quantidade ótima de PC's obtidas foi {PC_usada + 1}"
    #       )

    # Inicio da validação do ciclo externo
    df_treino = X.loc[treino, :]
    df_teste = X.loc[teste, :]
    y_treino = df_treino[classe]
    y_teste = df_teste[classe]
    df_treino = df_treino.drop(classe, axis = 1)
    df_teste = df_teste.drop(classe, axis = 1)
    mu = df_treino.mean()
    df_treino_c = df_treino - mu
    df_teste_c = df_teste - mu

    U, s, Vh = np.linalg.svd(df_treino_c, full_matrices= False)
    V = Vh.T
    scores_treino = df_treino_c.dot(V[:,0:PC_usada+1])
    scores_treino = pd.DataFrame(scores_treino)
    scores_treino[classe] = y_treino.values

    scores_teste = df_teste_c.dot(V[:,0:PC_usada+1])

    Sb, Sw, LDA = LDAcalc(scores_treino, classe, list(range(PC_usada+1)))
    w = LDA.iloc[0,:]
    scores_treino = scores_treino.drop(classe, axis = 1)
    LDA_treino = scores_treino.dot(w)
    y_true_treino = (y_treino == classe_sup).astype(float).to_numpy()
    if np.mean(LDA_treino[y_true_treino ==1]) < np.mean(LDA_treino[y_true_treino == 0 ]): w = -w


    LDA_teste = scores_teste.dot(w)
    y_true_teste = (y_teste == classe_sup).astype(float).to_numpy()
    auc = AUC(y_true_teste, LDA_teste)

    TP = np.sum((y_true_teste == 1 ) & (LDA_teste > 0))
    FP = np.sum((y_true_teste == 0 ) & (LDA_teste > 0))
    FN = np.sum((y_true_teste == 1) & (LDA_teste < 0))
    TN = np.sum((y_true_teste == 0) & (LDA_teste < 0))

    sensibilidade = TP / (TP + FN) if (TP + FN) > 0 else np.nan
    especificidade = TN / (TN + FP) if (TN + FP) > 0 else np.nan
    acuracia = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else np.nan
    PPV = TP / (TP + FP) if (TP + FP) > 0 else np.nan
    NPV = TN / (TN + FN) if (TN + FN) > 0 else np.nan
    matriz_de_parametros = [sensibilidade, especificidade, acuracia, PPV, NPV, auc]
    lista_parametros.append(matriz_de_parametros)
    matriz_de_confusao = [TP, FP, FN, TN]
    lista_confusao.append(matriz_de_confusao)

    lista_scores.append([np.median(scores_teste[y_true_teste == 1], axis = 0), np.median(scores_teste[y_true_teste == 0], axis = 0),PC_usada+1])






  return(historico_erros, historico_AUC, n_pcs, lista_parametros, lista_confusao, lista_scores)

df_final_medianas = pd.DataFrame(columns = ["1","2","3"])

df_final_medianas